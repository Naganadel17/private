{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naganadel17/private/blob/main/FaceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFZ7K-_FtC7_",
        "outputId": "84fcd37b-42c3-4502-b38c-fed6ab460193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGGHmdvOtGM6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def get_files(path='./', ext=('.png', '.jpeg', '.jpg')):\n",
        "    \"\"\" Get all image files \"\"\"\n",
        "    files = []\n",
        "    for e in ext:\n",
        "        files.extend(glob.glob(f'{path}/**/*{e}'))\n",
        "    files.sort(key=lambda p: (os.path.dirname(p), int(os.path.basename(p).split('.')[0])))\n",
        "    return files\n",
        "\n",
        "def to_rgb_and_save(path):\n",
        "    \"\"\" Some of the images may have RGBA mode \"\"\"\n",
        "    for p in path:\n",
        "        img = Image.open(p)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "            img.save(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWJ0Fpc3tGiC"
      },
      "outputs": [],
      "source": [
        "# 경로 설정\n",
        "ABS_PATH = '/content/drive/MyDrive/Colab Notebooks/facenet/'\n",
        "DATA_PATH = os.path.join(ABS_PATH, 'data')\n",
        "\n",
        "TRAIN_DIR = os.path.join(DATA_PATH, 'train_images')\n",
        "TEST_DIR = os.path.join(DATA_PATH, 'test_images')\n",
        "\n",
        "ALIGNED_TRAIN_DIR = TRAIN_DIR + '_cropped'\n",
        "ALIGNED_TEST_DIR = TEST_DIR + '_cropped'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmuk9ieltGmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77883a97-06ae-4812-ea3a-a724e11e94e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train files\n",
            "\tpath: /content/drive/MyDrive/Colab Notebooks/facenet/data/train_images\n",
            "\ttotal number: 86\n",
            "\t- Kim - 10\n",
            "\t- Kroos - 13\n",
            "\t- Marcelo - 14\n",
            "\t- Modric - 13\n",
            "\t- Perez - 12\n",
            "\t- Ramos - 13\n",
            "\t- Zidane - 11\n",
            "Train files\n",
            "\tpath: /content/drive/MyDrive/Colab Notebooks/facenet/data/test_images\n",
            "\ttotal number: 14\n",
            "\t- Kim - 2\n",
            "\t- Kroos - 2\n",
            "\t- Marcelo - 2\n",
            "\t- Modric - 2\n",
            "\t- Perez - 2\n",
            "\t- Ramos - 2\n",
            "\t- Zidane - 2\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "# TRAIN_DIR/TEST_DIR path 받기\n",
        "trainF, testF = get_files(TRAIN_DIR), get_files(TEST_DIR)\n",
        "\n",
        "# 각 파일별 사진 개수 표시\n",
        "trainC, testC = Counter(map(os.path.dirname, trainF)), Counter(map(os.path.dirname, testF))\n",
        "train_total, train_text  = sum(trainC.values()), '\\n'.join([f'\\t- {os.path.basename(fp)} - {c}' for fp, c in trainC.items()])\n",
        "test_total, test_text  = sum(testC.values()), '\\n'.join([f'\\t- {os.path.basename(fp)} - {c}' for fp, c in testC.items()])\n",
        "\n",
        "print(f'Train files\\n\\tpath: {TRAIN_DIR}\\n\\ttotal number: {train_total}\\n{train_text}')\n",
        "print(f'Train files\\n\\tpath: {TEST_DIR}\\n\\ttotal number: {test_total}\\n{test_text}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3XqsoHctGqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b52782-b243-4a34-81bb-a1eb968bc38d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "# 사진 RGB 컬러로 변경\n",
        "to_rgb_and_save(trainF), to_rgb_and_save(testF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ySbhAPStGur"
      },
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "%matplotlib inline\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "\n",
        "def imshow(img, ax, title):\n",
        "    ax.imshow(img)\n",
        "    if title:\n",
        "        el = Ellipse((2, -1), 0.5, 0.5)\n",
        "        ax.annotate(title, xy=(1, 0), xycoords='axes fraction', ha='right', va='bottom',\n",
        "                    bbox=dict(boxstyle=\"round\", fc=\"0.8\"),\n",
        "                    arrowprops=dict(arrowstyle=\"simple\", fc=\"0.6\", ec=\"none\",\n",
        "                                    patchB=el, connectionstyle=\"arc3, rad=0.3\"))\n",
        "    ax.set_xticks([]), ax.set_yticks([])\n",
        "\n",
        "def plot_gallery(images, ncols, nrows, titles=None, title='', figsize=None):\n",
        "    if figsize is None:\n",
        "        figsize = (18, ncols) if ncols < 10 else (18, 20)\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    grid = ImageGrid(fig, 111, nrows_ncols=(nrows, ncols), axes_pad=0.02)\n",
        "\n",
        "    for i, ax in enumerate(grid):\n",
        "        if i == len(images): break\n",
        "        imshow(images[i], ax, titles[i] if titles is not None else '')\n",
        "\n",
        "    # there are some problems with suptitle alignment\n",
        "    y_title_pos = grid[0].get_position().get_points()[1][1] - 0.33 / (1 if nrows == 1 else nrows / 3)\n",
        "    plt.suptitle(title, y=y_title_pos, fontsize=12)\n",
        "\n",
        "def plot(paths=None, images=None, titles=None, axtitle=True, title='', to_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Plot image gallery by passing (paths, title) or (images, titles)\n",
        "    :param paths: list of image paths\n",
        "    :param images: list of (PIL.Image | np.array | torch.Tensor) objects\n",
        "    :param titles: list of image titles\n",
        "    :param bool axtitle: if paths is not None, then axtitle=True leads to use basedir name as titles\n",
        "    :param str title: gallery title\n",
        "    :param to_size: image resizing size before plot, default (512, 512)\n",
        "    \"\"\"\n",
        "\n",
        "    if paths is not None and len(paths):\n",
        "        images = [Image.open(p).resize(to_size) for p in paths]\n",
        "\n",
        "        nrows = int(ceil(len(images) / 12)) # 12 images per row\n",
        "        ncols = 12 if nrows > 1 else len(images)\n",
        "\n",
        "        if axtitle:\n",
        "              titles = [os.path.dirname(p).split('/')[-1] for p in paths]\n",
        "\n",
        "        plot_gallery(images, ncols, nrows, titles, title)\n",
        "\n",
        "    elif images is not None and len(images):\n",
        "        if isinstance(images, list):\n",
        "            images = np.array(images)\n",
        "\n",
        "        nrows = int(ceil(len(images) / 12)) # 12 images per row\n",
        "        ncols = 12 if nrows > 1 else len(images)\n",
        "\n",
        "        # Rescale to [0., 1.]\n",
        "        if images[0].max() > 1:\n",
        "            images /= 255.\n",
        "\n",
        "        # if torch.Tensor change axes\n",
        "        if not isinstance(images, np.ndarray):\n",
        "            if images.size(1) == 3 or 1:\n",
        "                images = images.permute((0, 2, 3, 1))\n",
        "\n",
        "        plot_gallery(images, ncols, nrows, titles, title)\n",
        "\n",
        "\n",
        "    else:\n",
        "        raise LookupError('You didnt pass any path or image objects')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aUNrFxQtGzU"
      },
      "outputs": [],
      "source": [
        "# Train 이미지\n",
        "# plot(paths=trainF, title='Train images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hdAImQ2tG3r"
      },
      "outputs": [],
      "source": [
        "# Test 이미지\n",
        "# plot(paths=testF, title='Test images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5TjnlOHtG8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36ec864-b172-405c-a149-75f1b68cae0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# FaceNet 패키지 이용 (MTCNN,... 등등)\n",
        "# MTCNN : 얼굴 추출 사용 라이브러리\n",
        "# 얼굴 추출 라이브러리 비교 https://ichi.pro/ko/facenet-kerasleul-sayonghayeo-eolgul-insig-model-eul-mandeuneun-bangbeob-eun-mueos-ibnikka-278613466321649\n",
        "\n",
        "!pip install facenet-pytorch\n",
        "\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1, training, fixed_image_standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD3D2FP5tHAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6b1218-cf7a-4fe2-b93e-7b2803439fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# 진행 상태 표시 tqdm\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import shutil\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Running on device: {device}')\n",
        "\n",
        "\n",
        "def crop_face_and_save(path, new_path=None, model=MTCNN, transformer=None, params=None):\n",
        "    \"\"\"\n",
        "    Detect face on each image, crop them and save to \"new_path\"\n",
        "    :param str path: path with images will be passed to  datasets.ImageFolder\n",
        "    :param str new_path: path to locate new \"aligned\" images, if new_path is None\n",
        "                     then new_path will be path + \"_cropped\"\n",
        "    :param model: model to detect faces, default MTCNN\n",
        "    :param transformer: transformer object will be passed to ImageFolder\n",
        "    :param params: parameters of MTCNN model\n",
        "    \"\"\"\n",
        "    if not new_path:\n",
        "        new_path = path + '_cropped'\n",
        "    # if MTCNN 모델 오류\n",
        "    if os.path.exists(new_path):\n",
        "      shutil.rmtree(new_path)\n",
        "\n",
        "    # MTCNN 기본 파라미터\n",
        "    if not params:\n",
        "        params = {\n",
        "            'image_size': 160, 'margin': 0,\n",
        "            'min_face_size': 10, 'thresholds': [0.6, 0.7, 0.7],\n",
        "            'factor': 0.709, 'post_process': False, 'device': device\n",
        "            }\n",
        "\n",
        "    model = model(**params)\n",
        "    if not transformer:\n",
        "        transformer = transforms.Lambda(\n",
        "            lambd=lambda x: x.resize((1280, 1280)) if (np.array(x) > 2000).all() else x\n",
        "        )\n",
        "    print(transformer)\n",
        "    # file path 대신 ImageFolder를 이용해서 편리하게 사용\n",
        "    dataset = datasets.ImageFolder(path, transform=transformer)\n",
        "    dataset.samples = [(p, p.replace(path, new_path)) for p, _ in dataset.samples]\n",
        "\n",
        "    # batch size 1 as long as we havent exact image size and MTCNN will raise an error\n",
        "    loader = DataLoader(dataset, batch_size=1, collate_fn=training.collate_pil)\n",
        "    for i, (x, y) in enumerate(tqdm.tqdm(loader)):\n",
        "        model(x, save_path=y)\n",
        "    # 메모리 여유\n",
        "    del model, loader, dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjXhO5VltHFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9367997-5ad1-4674-df89-d706805a56ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t- Train data\n",
            "Lambda()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 86/86 [00:07<00:00, 11.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t- Test data\n",
            "Lambda()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.97it/s]\n"
          ]
        }
      ],
      "source": [
        "# train data 이미지 얼굴 추출 및 저장하기\n",
        "print('\\t- Train data')\n",
        "\n",
        "crop_face_and_save(TRAIN_DIR, ALIGNED_TRAIN_DIR)\n",
        "\n",
        "# 저장할 때 빠진 데이터가 없는 지 확인\n",
        "train_files, train_aligned_files = get_files(TRAIN_DIR), get_files(ALIGNED_TRAIN_DIR)\n",
        "if len(train_files) != len(train_aligned_files):\n",
        "    files = set(map(lambda fp: os.path.relpath(fp, start=TRAIN_DIR), train_files))\n",
        "    aligned_files = set(map(lambda fp: os.path.relpath(fp, start=ALIGNED_TRAIN_DIR), train_aligned_files))\n",
        "    detect_failed_train_files = list(files - aligned_files)\n",
        "    print(f\"\\nfiles {len(aligned_files)}/{len(files)}: {', '.join(detect_failed_train_files)} were not saved\")\n",
        "\n",
        "# -------------                     -------------\n",
        "\n",
        "# test data 이미지 얼굴 추출\n",
        "print('\\t- Test data')\n",
        "crop_face_and_save(TEST_DIR, ALIGNED_TEST_DIR)\n",
        "\n",
        "# 저장할 때 빠진 데이터가 없는 지 확인\n",
        "test_files, test_aligned_files = get_files(TEST_DIR), get_files(ALIGNED_TEST_DIR)\n",
        "if len(test_files) != len(test_aligned_files):\n",
        "    files = set(map(lambda fp: os.path.relpath(fp, start=TEST_DIR), test_files))\n",
        "    aligned_files = set(map(lambda fp: os.path.relpath(fp, start=ALIGNED_TEST_DIR), test_aligned_files))\n",
        "    detect_failed_test_files = list(files - aligned_files)\n",
        "    print(f\"\\nfiles {len(aligned_files)}/{len(files)}: {', '.join(detect_failed_train_files)} were not saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So23e02otHJe"
      },
      "outputs": [],
      "source": [
        "# train data 얼굴만 추출\n",
        "trainF = get_files(ALIGNED_TRAIN_DIR)\n",
        "# plot(paths=trainF, title='Aligned train images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4EqPU86tHN0"
      },
      "outputs": [],
      "source": [
        "# test data 얼굴만 추출\n",
        "testF = get_files(ALIGNED_TEST_DIR)\n",
        "# plot(paths=testF, title='Aligned test images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_wLOhXbtHV8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "f07bc230-d1f3-44a2-82fe-317efc5ec7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.9.0.80)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.11.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (10.2.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (24.0)\n",
            "Collecting pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 (from scikit-image>=0.16.1->albumentations)\n",
            "  Using cached Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.5.0)\n",
            "Installing collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.2.0\n",
            "    Uninstalling pillow-10.2.0:\n",
            "      Successfully uninstalled pillow-10.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires Pillow<10.3.0,>=10.2.0, but you have pillow 10.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pillow-10.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "1250691fe3d54a41868918a7ee2cea88"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install albumentations\n",
        "\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH_6gU8tHaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745e09f8-dda0-40b8-b018-1ba9c1382eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/transforms.py:1284: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/transforms.py:311: FutureWarning: JpegCompression has been deprecated. Please use ImageCompression\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from facenet_pytorch import fixed_image_standardization\n",
        "\n",
        "standard_transform = transforms.Compose([\n",
        "                                np.float32,\n",
        "                                transforms.ToTensor(),\n",
        "                                fixed_image_standardization\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "aug_mask = A.Compose([\n",
        "                   A.HorizontalFlip(p=0.5),\n",
        "                   A.VerticalFlip(p=0.15),\n",
        "                   A.RandomContrast(limit=0.5, p=0.4),\n",
        "                   A.Rotate(30, p=0.2),\n",
        "                   A.RandomSizedCrop((120, 120), 160, 160, p=0.4),\n",
        "                   A.OneOrOther(A.JpegCompression(p=0.2), A.Blur(p=0.2), p=0.66),\n",
        "                   A.OneOf([\n",
        "                            A.Rotate(45, p=0.3),\n",
        "                            A.ElasticTransform(sigma=20, alpha_affine=20, border_mode=0, p=0.2)\n",
        "                            ], p=0.5),\n",
        "                  A.HueSaturationValue(val_shift_limit=10, p=0.3)\n",
        "], p=1)\n",
        "\n",
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "                                 transforms.Lambda(lambd=lambda x: aug_mask(image=np.array(x))['image']),\n",
        "                                 standard_transform\n",
        "    ]),\n",
        "    'test': standard_transform\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vydxOisAtHeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "708e66a7-d6f9-45a2-b58d-331a9d91e4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALIGNED_TRAIN_DIR =  /content/drive/MyDrive/Colab Notebooks/facenet/data/train_images_cropped\n",
            "ALIGNED_TRAIN_DIR =  /content/drive/MyDrive/Colab Notebooks/facenet/data/test_images_cropped\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "\n",
        "b = 32\n",
        "\n",
        "# Original train 이미지\n",
        "print('ALIGNED_TRAIN_DIR = ',ALIGNED_TRAIN_DIR)\n",
        "print('ALIGNED_TRAIN_DIR = ',ALIGNED_TEST_DIR)\n",
        "trainD = datasets.ImageFolder(ALIGNED_TRAIN_DIR, transform=standard_transform)\n",
        "\n",
        "# Augmented train 이미지\n",
        "trainD_aug = datasets.ImageFolder(ALIGNED_TRAIN_DIR, transform=transform['train'])\n",
        "# Train Loader\n",
        "trainL = DataLoader(trainD, batch_size=b, num_workers=2) # x: torch.Size([batch_size, 3, 160, 160]), y: torch.Size([batch_size])\n",
        "\n",
        "\n",
        "trainL_aug = DataLoader(trainD_aug, batch_size=b, num_workers=2)\n",
        "\n",
        "# Original test 이미지\n",
        "testD = datasets.ImageFolder(ALIGNED_TEST_DIR, transform=standard_transform)\n",
        "# Test Loader\n",
        "testL = DataLoader(testD, batch_size=b, num_workers=2)\n",
        "\n",
        "\n",
        "# 이름 클래스들로 변경\n",
        "IDX_TO_CLASS = np.array(list(trainD.class_to_idx.keys()))\n",
        "CLASS_TO_IDX = dict(trainD.class_to_idx.items())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = 108\n",
        "trainD = datasets.ImageFolder(ALIGNED_TRAIN_DIR, transform=standard_transform)\n",
        "trainL = DataLoader(trainD, batch_size=b, num_workers=2)\n",
        "        #  DataLoader(trainD_aug, batch_size=b, num_workers=2)\n",
        "testD = datasets.ImageFolder(ALIGNED_TEST_DIR, transform=standard_transform)\n",
        "testL = DataLoader(testD, batch_size=b, num_workers=2)"
      ],
      "metadata": {
        "id": "S14-Xg1LuFuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUyoVQfktHiy"
      },
      "outputs": [],
      "source": [
        "# facenet의 InceptionResnetV1 모듈 불러오기\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "# model에 InceptionResnetV1 할당 (vggface로 pretrained)\n",
        "model = InceptionResnetV1(pretrained='vggface2', dropout_prob=0.5, device=device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5ZD9m7LtHm1"
      },
      "outputs": [],
      "source": [
        "def fixed_denormalize(image):\n",
        "    \"\"\" Restandartize images to [0, 255]\"\"\"\n",
        "    return image * 128 + 127.5\n",
        "\n",
        "from datetime import datetime\n",
        "def getEmbeds(model, n, loader, imshow=False, n_img=6):\n",
        "    model.eval()\n",
        "    # 보여줄 이미지\n",
        "    images = []\n",
        "\n",
        "    embeds, labels = [], []\n",
        "    print(datetime.now())\n",
        "    np.random.seed(int(datetime.utcnow().timestamp()))\n",
        "    for n_i in tqdm.trange(n):\n",
        "        for i, (x, y) in enumerate(loader, 1):\n",
        "\n",
        "            # # on each first batch get 'n_img' images\n",
        "            # if imshow and i == 1:\n",
        "            #     inds = np.random.choice(x.size(0), min(x.size(0), n_img))\n",
        "            #     print(\"*************\",inds)\n",
        "            #     images.append(fixed_denormalize(x[inds].data.cpu()).permute((0, 2, 3, 1)).numpy())\n",
        "\n",
        "            embed = model(x.to(device))\n",
        "            embed = embed.data.cpu().numpy()\n",
        "            embeds.append(embed), labels.extend(y.data.cpu().numpy())\n",
        "\n",
        "    if imshow:\n",
        "        plot(images=np.concatenate(images))\n",
        "\n",
        "    return np.concatenate(embeds), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩하기\n",
        "# Train embeddings\n",
        "trainEmbeds, trainLabels = getEmbeds(model, 50, trainL, False)\n",
        "trainEmbeds_aug, trainLabels_aug = getEmbeds(model, 50, trainL_aug, imshow=False)#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACfZLj6boknS",
        "outputId": "da2f5637-e816-447d-96b4-edc08702ffe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-30 12:17:00.885048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:53<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-30 12:17:54.856860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFAxXvUNtHqV"
      },
      "outputs": [],
      "source": [
        "trainEmbeds = np.concatenate([trainEmbeds, trainEmbeds_aug])\n",
        "trainLabels = np.concatenate([trainLabels, trainLabels_aug])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test embeddings\n",
        "testEmbeds, testLabels = getEmbeds(model, 1, testL, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpiRYkeEk96e",
        "outputId": "3ead4618-a5c6-494c-c5d3-ed099d8d76c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-30 12:18:42.679450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9FeBHVZtHtn"
      },
      "outputs": [],
      "source": [
        "# 임베딩 npz 파일로 저장\n",
        "TRAIN_EMBEDS = os.path.join(DATA_PATH, 'trainEmbeds.npz')\n",
        "TEST_EMBEDS = os.path.join(DATA_PATH, 'testEmbeds.npz')\n",
        "\n",
        "np.savez(TRAIN_EMBEDS, x=trainEmbeds, y=trainLabels)\n",
        "np.savez(TEST_EMBEDS, x=testEmbeds, y=testLabels)\n",
        "\n",
        "# 임베딩된 npz 파일 불러오기\n",
        "trainEmbeds, trainLabels = np.load(TRAIN_EMBEDS, allow_pickle=True).values()\n",
        "testEmbeds, testLabels = np.load(TEST_EMBEDS, allow_pickle=True).values()\n",
        "\n",
        "# 이름 라벨링\n",
        "trainLabels, testLabels = IDX_TO_CLASS[trainLabels], IDX_TO_CLASS[testLabels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc-OFVEDtIBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a939632-d3b6-41e3-9af3-2d641b2972fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train embeds size: (8600, 512)\n",
            "Tagret train size: (8600,)\n"
          ]
        }
      ],
      "source": [
        "# data preparation\n",
        "X = np.copy(trainEmbeds)\n",
        "y = np.array([CLASS_TO_IDX[label] for label in trainLabels])\n",
        "\n",
        "print(f'X train embeds size: {X.shape}')\n",
        "print(f'Tagret train size: {y.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drha-YtdtID9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ffee51-8bc6-4dfc-b6a1-96532d9ead41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best estimator:  SVC(C=1000.0, class_weight='balanced', gamma=0.01, kernel='sigmoid',\n",
            "    max_iter=30, probability=True, random_state=3)\n",
            "Best params:  {'kernel': 'sigmoid', 'gamma': 0.01, 'C': 1000.0}\n"
          ]
        }
      ],
      "source": [
        "# SVC 모델 사용하기\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', 'Solver terminated early.*')\n",
        "# svc 파라미터\n",
        "param_grid = {'C': [1, 10, 100, 1e3, 5e3, 1e4, 5e4, 1e5],\n",
        "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 'auto'],\n",
        "              'kernel': ['rbf', 'sigmoid', 'poly']}\n",
        "model_params = {'class_weight': 'balanced', 'max_iter': 30, 'probability': True, 'random_state': 3}\n",
        "model = SVC(**model_params)\n",
        "# GridSearch로 최적의 파라미터 구하기\n",
        "clf = RandomizedSearchCV(model, param_grid)\n",
        "clf.fit(X, y)\n",
        "\n",
        "print('Best estimator: ', clf.best_estimator_)\n",
        "print('Best params: ', clf.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(clf))\n",
        "print(type(clf.best_estimator_))\n",
        "# 모델에 최적의 파라미터 적용\n",
        "clf = clf.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLLyXoCTuJK1",
        "outputId": "495b40a4-0ef3-4c9b-a21f-6f6aaa5e7b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
            "<class 'sklearn.svm._classes.SVC'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FhRdvyUtIHA"
      },
      "outputs": [],
      "source": [
        "import joblib as joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aBbBA32tIKJ"
      },
      "outputs": [],
      "source": [
        "SVM_PATH = os.path.join(DATA_PATH, 'svm.sav')\n",
        "joblib.dump(clf, SVM_PATH)\n",
        "clf = joblib.load(SVM_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biuWn9DmtIM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af5f6b7-c281-49ca-efe9-a8ce84b7ea50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train embeds size: (14, 512)\n",
            "Tagret train size: (14,)\n"
          ]
        }
      ],
      "source": [
        "# test data 준비\n",
        "X_test, y_test = np.copy(testEmbeds), np.array([CLASS_TO_IDX[label] for label in testLabels])\n",
        "print(f'X train embeds size: {X_test.shape}')\n",
        "print(f'Tagret train size: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk3CE6HJtIP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0e7e72-f759-48de-bafc-1a83ab5d0172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score on train data: 0.955\n",
            "Accuracy score on test data: 0.8571428571428571\n"
          ]
        }
      ],
      "source": [
        "# 정확도 측정하기\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "inds = range(88)\n",
        "train_acc = accuracy_score(clf.predict(X[inds]), y[inds])\n",
        "print(f'Accuracy score on train data: {train_acc:.3f}')\n",
        "\n",
        "test_acc = accuracy_score(clf.predict(X_test), y_test)\n",
        "print(f'Accuracy score on test data: {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvAWEA5PtITU"
      },
      "outputs": [],
      "source": [
        "# gif 파일 전환 함수 설정\n",
        "import imageio\n",
        "\n",
        "def toGif(path, dim):\n",
        "    gpath = ''.join(path.split('.')[:-1]) + '.gif'\n",
        "\n",
        "    with imageio.get_writer(gpath, mode='I') as writer:\n",
        "        frames = []\n",
        "        capture = cv2.VideoCapture(path)\n",
        "\n",
        "        i = 0\n",
        "        while True:\n",
        "            ret, frame = capture.read()\n",
        "            if not ret: break\n",
        "\n",
        "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            writer.append_data(cv2.resize(image, dim))\n",
        "            i += 1\n",
        "        print(f'Total frames: {i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO4pfOHAtIYb"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 비디오를 Gif로 변환\n",
        "VIDEO_PATH = os.path.join(DATA_PATH, 'video/')\n",
        "width, height = 640, 360\n",
        "\n",
        "mov1 = os.path.join(VIDEO_PATH, '2.mp4')\n",
        "toGif(mov1, (width, height))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqrB2farvhEy",
        "outputId": "27734b08-1d1e-4c7a-c106-b9e5149299f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames: 285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def diag(x1, y1, x2, y2):\n",
        "    return np.linalg.norm([x2 - x1, y2 - y1])\n",
        "\n",
        "\n",
        "def square(x1, y1, x2, y2):\n",
        "    return abs(x2 - x1) * abs(y2 - y1)\n",
        "\n",
        "\n",
        "def isOverlap(rect1, rect2):\n",
        "    x1, x2 = rect1[0], rect1[2]\n",
        "    y1, y2 = rect1[1], rect1[3]\n",
        "\n",
        "    x1_, x2_ = rect2[0], rect2[2]\n",
        "    y1_, y2_ = rect2[1], rect2[3]\n",
        "\n",
        "    if x1 > x2_ or x2 < x1_: return False\n",
        "    if y1 > y2_ or y2 < y1_: return False\n",
        "\n",
        "    rght, lft = x1 < x1_ < x2, x1_ < x1 < x2_\n",
        "    d1, d2 = 0, diag(x1_, y1_, x2_, y2_)\n",
        "    threshold = 0.5\n",
        "\n",
        "    if rght and y1 < y1_: d1 = diag(x1_, y1_, x2, y2)\n",
        "    elif rght and y1 > y1_: d1 = diag(x1_, y2_, x2, y1)\n",
        "    elif lft and y1 < y1_: d1 = diag(x2_, y1_, x1, y2)\n",
        "    elif lft and y1 > y1_: d1 = diag(x2_, y2_, x1, y1)\n",
        "\n",
        "    if d1 / d2 >= threshold and square(x1, y1, x2, y2) < square(x1_, y1_, x2_, y2_): return True\n",
        "    return False\n",
        "\n",
        "def draw_box(draw, boxes, names, probs, min_p=0.89):\n",
        "    font = ImageFont.truetype(os.path.join(ABS_PATH, 'arial.ttf'), size=22)\n",
        "\n",
        "    not_overlap_inds = []\n",
        "    for i in range(len(boxes)):\n",
        "        not_overlap = True\n",
        "        for box2 in boxes:\n",
        "            if np.all(boxes[i] == box2): continue\n",
        "            not_overlap = not isOverlap(boxes[i], box2)\n",
        "            if not not_overlap: break\n",
        "        if not_overlap: not_overlap_inds.append(i)\n",
        "\n",
        "    boxes = [boxes[i] for i in not_overlap_inds]\n",
        "    probs = [probs[i] for i in not_overlap_inds]\n",
        "    for box, name, prob in zip(boxes, names, probs):\n",
        "        if prob >= min_p:\n",
        "            draw.rectangle(box.tolist(), outline=(255, 255, 255), width=5)\n",
        "            x1, y1, _, _ = box\n",
        "            bbox = font.getbbox(f'{name}')\n",
        "            text_width = bbox[2] - bbox[0]\n",
        "            text_height = bbox[3] - bbox[1]\n",
        "            draw.rectangle(((x1, y1 - text_height), (x1 + text_width, y1)), fill='white')\n",
        "            draw.text((x1, y1 - text_height), f'{name}: {prob:.2f}', (24, 12, 30), font)\n",
        "\n",
        "    return boxes, probs"
      ],
      "metadata": {
        "id": "U462e7gLvhKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standard_transform = transforms.Compose([\n",
        "                                transforms.Resize((160, 160)),\n",
        "                                np.float32,\n",
        "                                transforms.ToTensor(),\n",
        "                                fixed_image_standardization\n",
        "])\n",
        "\n",
        "def get_video_embedding(model, x):\n",
        "    embeds = model(x.to(device))\n",
        "    return embeds.detach().cpu().numpy()\n",
        "\n",
        "def face_extract(model, clf, frame, boxes):\n",
        "    names, prob = [], []\n",
        "    if len(boxes):\n",
        "        x = torch.stack([standard_transform(frame.crop(b)) for b in boxes])\n",
        "        embeds = get_video_embedding(model, x)\n",
        "        idx, prob = clf.predict(embeds), clf.predict_proba(embeds).max(axis=1)\n",
        "        names = [IDX_TO_CLASS[idx_] for idx_ in idx]\n",
        "    return names, prob\n",
        "\n",
        "def preprocess_image(detector, face_extractor, clf, path, transform=None):\n",
        "    if not transform: transform = lambda x: x.resize((1280, 1280)) if (np.array(x.size) > 2000).all() else x\n",
        "    capture = Image.open(path).convert('RGB')\n",
        "    i = 0\n",
        "\n",
        "    # iframe = Image.fromarray(transform(np.array(capture)))\n",
        "    iframe = transform(capture)\n",
        "\n",
        "    boxes, probs = detector.detect(iframe)\n",
        "    if boxes is None: boxes, probs = [], []\n",
        "    names, prob = face_extract(face_extractor, clf, iframe, boxes)\n",
        "\n",
        "    frame_draw = iframe.copy()\n",
        "    draw = ImageDraw.Draw(frame_draw)\n",
        "\n",
        "    boxes, probs = draw_box(draw, boxes, names, probs)\n",
        "    return frame_draw.resize((620, 480), Image.BILINEAR)\n",
        "\n",
        "\n",
        "def preprocess_video(detector, face_extractor, clf, path, transform=None, k=3):\n",
        "    frames = []\n",
        "    if not transform: transform = lambda x: x.resize((1280, 1280)) if (np.array(x.shape) > 2000).all() else x\n",
        "    capture = cv2.VideoCapture(path)\n",
        "    i = 0\n",
        "    while True:\n",
        "        ret, frame = capture.read()\n",
        "        if not ret: break\n",
        "\n",
        "        iframe = Image.fromarray(transform(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "\n",
        "        if (i + 1) % k:\n",
        "            boxes, probs = detector.detect(iframe)\n",
        "            if boxes is None: boxes, probs = [], []\n",
        "            names, prob = face_extract(face_extractor, clf, iframe, boxes)\n",
        "\n",
        "        frame_draw = iframe.copy()\n",
        "        draw = ImageDraw.Draw(frame_draw)\n",
        "\n",
        "        boxes, probs = draw_box(draw, boxes, names, probs)\n",
        "        frames.append(frame_draw.resize((620, 480), Image.BILINEAR))\n",
        "        i += 1\n",
        "\n",
        "    print(f'Total frames: {i}')\n",
        "    return frames\n",
        "\n",
        "def framesToGif(frames, path):\n",
        "    with imageio.get_writer(path, mode='I') as writer:\n",
        "        for frame in tqdm.tqdm(frames):\n",
        "            writer.append_data(np.array(frame))"
      ],
      "metadata": {
        "id": "cHBj64xDvhQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 폰트 (*꼭 지정해줘야함, 안하면 작동 X, arial.ttf 폰트 다운받아서 ABS_PATH에 넣어주면 됨)\n",
        "from PIL import ImageFont\n",
        "\n",
        "k = 3 # each k image will be processed by networks\n",
        "\n",
        "font = ImageFont.truetype(os.path.join(ABS_PATH, 'arial.ttf'), size=22)\n",
        "\n",
        "mtcnn = MTCNN(keep_all=True, min_face_size=70, device=device)\n",
        "model = InceptionResnetV1(pretrained='vggface2', dropout_prob=0.6, device=device).eval()"
      ],
      "metadata": {
        "id": "A8YlfTsVvhWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "# 결과 동영상 1_aug.gif 로 저장\n",
        "print('Processing mov1: ')\n",
        "frames = preprocess_video(mtcnn, model, clf, mov1)\n",
        "mov1_aug = os.path.join(VIDEO_PATH, '1_aug.gif')\n",
        "framesToGif(frames, mov1_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csAvflrpvhcB",
        "outputId": "abff260e-fad8-4b4c-e419-eb87a718960d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing mov1: \n",
            "Total frames: 285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 285/285 [00:00<00:00, 1037.94it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}